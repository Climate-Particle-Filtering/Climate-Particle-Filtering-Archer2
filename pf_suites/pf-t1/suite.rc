#!jinja2
[cylc]
    UTC mode = True 
    # Timeout handlers
    [[events]]
        timeout = P1D

[scheduling]
    initial cycle point = now

    [[dependencies]]
        [[[R1]]]
             graph = startclones
        [[[PT30M]]]
             graph = sendreq[-PT30M] => getreq => sendreq 
        
[runtime]
# Root, inherited by everything
    [[root]]
        init-script = """
export CYLC_VERSION={{CYLC_VERSION}}
export ROSE_VERSION={{ROSE_VERSION}}
"""
        script = "rose task-run --verbose"
        [[[events]]]
            # Cylc has sensible defaults for event notification- only add
            # to the entry below if you want to be notified by mail
            mail events = submission failed, submission timeout, submission retry, retry, failed, timeout
            submission timeout = P1D # 1 day
        [[[environment]]]
            ROSE_ORIG_HOST={{ ROSE_ORIG_HOST }}
            RUNS = unset
               # following set in suite startup command --define...
            PF_BASE_SUITE = {{ PF_BASE_SUITE }} 
            PF_ENS_SIZE = {{ PF_ENS_SIZE }} 
            PF_ENS_DIR = {{ PF_ENS_DIR }} 
            PFname=$(basename $PF_ENS_DIR )
            PF_ENS_PREFIX = ${PFname:0:2}
     [[HPC_ALL]] 
        [[[directives]]]
            --export = none
            --account = {{ HPC_ACCOUNT }}
        [[[job]]]
            batch system = slurm
        [[[remote]]]
            host = {{ HPC_HOST }}
            owner = {{ HPC_USER }}

     [[HPC_SERIAL]]
        inherit = None, HPC_ALL
        [[[environment]]]
            ROSE_TASK_N_JOBS = 1
        [[[job]]]
            execution time limit = PT20M
        [[[directives]]]
            --partition=serial
            --qos=serial
            --ntasks=1
            --nodes = 1
            --cpus-per-task = 1
            --mem=4G

    [[LINUX]]
        [[[remote]]]
            host = {{ ROSE_ORIG_HOST }}
        [[[job]]]
            batch system = background

    [[sendreq]]
        inherit = HPC_SERIAL
        script = """
                 hostname
                 echo ens dir ${PF_ENS_DIR} with prefix ${PF_ENS_PREFIX} for size ${PF_ENS_SIZE}
                   # PF setup??
                 . {{GEOSM_SETUP_A2}}
                         # scan run directories to find runs needing a cloned base suite
                 if [[ -d ${PF_ENS_DIR} ]]
                 then
                      echo A2R=$({{PF_GET_RUNS}} ${PF_ENS_DIR})
                     echo A2R: $A2R
                     A2R=""
                     if [[ x${A2R} != x ]] 
                     then
                           for ia in ${A2R}
                           do
                              #PF_STUDY=$(basename {{PF_ENS_DIR}})

                              echo {{PF_SEND_RUNS}} -b {{PF_BASE_SUITE}} -D {{PF_ENS_DIR}} -s {{PF_STUDY}} -r ${ia}
#                             # {{PF_SEND_RUNS}} -b {{PF_BASE_SUITE}} -D {{PF_ENS_DIR}} -s {{PF_STUDY}} -r ${ia} 
                            #  touch {{OPT_STUDY_DIR}}/${ia}/state".QUEUED"
                           done
                     else
                      echo null in A2R
                     fi
                 else
                     echo no PF dir exists
                     exit 1
                 fi # may be called before stdydir ecsts,.

        """
        [[[job]]]
            execution time limit = PT20M

        [[[environment]]]
            ROSE_TASK_APP    = sendreq
            PREBUILD =
            ROSE_TASK_N_JOBS = 1
            ROSE_TASK_OPTIONS = --ignore-lock

    [[startclones]]
               # clone and start ech model in the ensemble using the stub directory name, zd001
        inherit = LINUX
        script = """
                 echo ens dir ${PF_ENS_DIR} with prefix ${PF_ENS_PREFIX} for size ${PF_ENS_SIZE}
                   # PF setup??
                 i=0
                 while [[ $i -lt ${PF_ENS_SIZE} ]]
                 do
                    new_suite=${PF_ENS_PREFIX}$(printf "%03d" $i)
                    rundir=${PF_ENS_DIR}/${new_suite}   # archer2 directory for the run
                          # start the suite for this run.
                          # never trying to do more runs than we have quota/queue space for.
                          # how does user perturb runs - prerun task in model suite.
                          # in optclim we had to wait to know wht runs are needed - here 
                          # its simpler - just kn ow hte ensemble size so start them then
                          # wait for their completion and readiness for continuation run.

                    cd /home/$USER/roses

                    if [[ -d $new_suite ]]
                    then
          # force uniqueness - might be unnecessary - consider later! Helps testing.
          # $$ is the process number of this process. Could use _1,_2 etc

                            new_suite=${new_suite}_$$
                    fi 
                    cp -r $PF_BASE_SUITE  $new_suite
                    cd $new_suite

               
                   df1="PF_RUNDIR='"${rundir}"'"
                   df2="PF_ENS_DIR='"${PF_ENS_DIR}"'"
                   echo rose suite-run --no-gcontrol --define-suite="${df1}" --define-suite="${df2}"
                   rose suite-run --no-gcontrol --define-suite="${df1}" --define-suite="${df2}"

                    cd /home/$USER/roses
                    ((i = i + 1 )) 
                 done
        """
        [[[job]]]
            execution time limit = PT20M

        [[[environment]]]
            ROSE_TASK_APP    = getreq
            PREBUILD =
            ROSE_TASK_N_JOBS = 1
            ROSE_TASK_OPTIONS = --ignore-lock
    [[getreq]]
        inherit = LINUX
        script = """
                 . {{GEOSM_SETUP_PUMA}}
                 PF_STUDY=$(basename {{PF_ENS_DIR}})
                 {{PF_PUMA_RECV_AND_CLONE}} -s {{PF_STUDY}} -C /home/mjm/geosmeta.cfg
        """
        [[[job]]]
            execution time limit = PT20M

        [[[environment]]]
            ROSE_TASK_APP    = getreq
            PREBUILD =
            ROSE_TASK_N_JOBS = 1
            ROSE_TASK_OPTIONS = --ignore-lock

